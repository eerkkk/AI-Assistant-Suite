import commonApiService from "@/services/api-service/common-api-service";
import { PromptTemplate, type OpenAI } from "langchain";
import {
  OutputFixingParser,
  StructuredOutputParser,
} from "langchain/output_parsers";
import z from "zod";

export interface SummaryParameters {
  textToSummarise: string;
  characterCount: number;
}

class SummariseLLMQueryService {
  generateSummariserParser = () =>
    StructuredOutputParser.fromZodSchema(
      z.object({
        summary: z
          .string()
          .describe("summary text generated by the summariser model"),
      })
    );

  generateSummaryPromptTemplate = () => {
    const parser = this.generateSummariserParser();
    const formatInstructions = parser.getFormatInstructions();

    return new PromptTemplate({
      template: `
    Summarize the following text in your own words. Incorporate the summary of previous text into your summary.
    Keep your summary within {characterCount} characters.

    ## Previous Text Summary

    {previousSummary}

    TLDR

    {textToSummarise}

    {format_instructions}
    `,
      inputVariables: ["textToSummarise", "characterCount", "previousSummary"],
      partialVariables: { format_instructions: formatInstructions },
    });
  };

  escapeDoubleQuotes = (jsonString: string): string => {
    try {
      // eslint-disable-next-line @typescript-eslint/no-unsafe-assignment
      const jsonObject: { summary?: string } = JSON.parse(jsonString);
      if (jsonObject.summary) {
        jsonObject.summary = jsonObject.summary.replace(/"/g, '\\"');
        jsonObject.summary.replace(/\n/g, "\\n");
        jsonObject.summary
          .replace(/\\/g, "\\\\")
          .replace(/\n/g, "\\n")
          .replace(/"/g, '\\"');
        return JSON.stringify(jsonObject);
      } else {
        return jsonString; // If "summary" key is not present, return the original JSON string as is.
      }
    } catch (error) {
      // If there's an error parsing the JSON, return the original JSON string as is.
      return jsonString;
    }
  };

  getSummaryByChunks = async (
    queryChunks: string[],
    prompt: PromptTemplate,
    characterCount: number,
    model: OpenAI
  ) => {
    const parser = this.generateSummariserParser();
    let summary = "";

    for (const chunk of queryChunks) {
      const inputParams = {
        textToSummarise: chunk,
        characterCount: characterCount / queryChunks.length,
        previousSummary: summary,
      };
      const llmInput = await commonApiService.generateFormattedLLMInput(
        prompt,
        inputParams
      );
      const llmResponse = (await model.call(llmInput)).trim();

      const llmResponse2 = this.escapeDoubleQuotes(llmResponse);

      try {
        const response = await parser.parse(llmResponse2);
        summary = response.summary;
      } catch (err) {
        const fixingParser = OutputFixingParser.fromLLM(model, parser);
        const fixedResponse = await fixingParser.parse(llmResponse2);

        summary = fixedResponse.summary;
      }
    }

    return { response: summary };
  };

  generateSummary = async (summaryParameters: SummaryParameters) => {
    const { characterCount, textToSummarise } = summaryParameters;

    const model = commonApiService.generateModel();

    const prompt = this.generateSummaryPromptTemplate();

    const baseInputParams = {
      textToSummarise: ".",
      characterCount: characterCount,
      previousSummary: "",
    };
    const baseLLMInput = await commonApiService.generateFormattedLLMInput(
      prompt,
      baseInputParams
    );

    const queryChunks = await commonApiService.generateQueryChunks(
      model,
      baseLLMInput,
      textToSummarise
    );

    return this.getSummaryByChunks(queryChunks, prompt, characterCount, model);
  };
}

const summariseLLMQueryService = new SummariseLLMQueryService();
export default summariseLLMQueryService;
